import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# -----------------------------
# CONFIG
# -----------------------------
LATENT_DIM = 20
BATCH_SIZE = 128
EPOCHS = 30
BETAS = [0.5, 1.0, 2.0, 4.0]
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# -----------------------------
# DATA
# -----------------------------
transform = transforms.ToTensor()

train_data = datasets.MNIST(
    root="./data", train=True, transform=transform, download=True
)

train_loader = DataLoader(
    train_data, batch_size=BATCH_SIZE, shuffle=True
)

# -----------------------------
# BASELINE AUTOENCODER
# -----------------------------
class AutoEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(784, 400),
            nn.ReLU(),
            nn.Linear(400, LATENT_DIM)
        )
        self.decoder = nn.Sequential(
            nn.Linear(LATENT_DIM, 400),
            nn.ReLU(),
            nn.Linear(400, 784),
            nn.Sigmoid()
        )

    def forward(self, x):
        z = self.encoder(x)
        return self.decoder(z)

# -----------------------------
# VARIATIONAL AUTOENCODER
# -----------------------------
class BetaVAE(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 400)
        self.fc_mu = nn.Linear(400, LATENT_DIM)
        self.fc_logvar = nn.Linear(400, LATENT_DIM)
        self.fc2 = nn.Linear(LATENT_DIM, 400)
        self.fc3 = nn.Linear(400, 784)

    def encode(self, x):
        h = torch.relu(self.fc1(x))
        return self.fc_mu(h), self.fc_logvar(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        h = torch.relu(self.fc2(z))
        return torch.sigmoid(self.fc3(h))

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

# -----------------------------
# LOSS FUNCTIONS
# -----------------------------
def ae_loss(recon, x):
    return nn.functional.binary_cross_entropy(recon, x, reduction='sum')

def vae_loss(recon, x, mu, logvar, beta):
    recon_loss = nn.functional.binary_cross_entropy(recon, x, reduction='sum')
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_loss + beta * kl_loss, recon_loss, kl_loss

# -----------------------------
# TRAIN AUTOENCODER
# -----------------------------
print("\nðŸ”¹ Training Baseline AutoEncoder")
ae = AutoEncoder().to(DEVICE)
optimizer_ae = optim.Adam(ae.parameters(), lr=1e-3)

for epoch in range(EPOCHS):
    total_loss = 0
    for x, _ in train_loader:
        x = x.view(-1, 784).to(DEVICE)
        optimizer_ae.zero_grad()
        recon = ae(x)
        loss = ae_loss(recon, x)
        loss.backward()
        optimizer_ae.step()
        total_loss += loss.item()

    print(f"Epoch [{epoch+1}/{EPOCHS}] AE Loss: {total_loss/len(train_loader.dataset):.4f}")

# -----------------------------
# TRAIN BETA-VAE (TUNING)
# -----------------------------
results = {}

for beta in BETAS:
    print(f"\nðŸ”¹ Training Beta-VAE with beta = {beta}")
    vae = BetaVAE().to(DEVICE)
    optimizer_vae = optim.Adam(vae.parameters(), lr=1e-3)

    for epoch in range(EPOCHS):
        recon_total, kl_total = 0, 0

        for x, _ in train_loader:
            x = x.view(-1, 784).to(DEVICE)
            optimizer_vae.zero_grad()

            recon, mu, logvar = vae(x)
            loss, recon_loss, kl_loss = vae_loss(recon, x, mu, logvar, beta)

            loss.backward()
            optimizer_vae.step()

            recon_total += recon_loss.item()
            kl_total += kl_loss.item()

        print(
            f"Epoch [{epoch+1}/{EPOCHS}] "
            f"Recon: {recon_total/len(train_loader.dataset):.4f} "
            f"KL: {kl_total/len(train_loader.dataset):.4f}"
        )

    results[beta] = (recon_total, kl_total)

# -----------------------------
# COMPARATIVE ANALYSIS
# -----------------------------
print("\nðŸ“Š Comparative Analysis (Beta Impact)")
for beta, (recon, kl) in results.items():
    print(
        f"Beta={beta} â†’ "
        f"Reconstruction Error={recon/len(train_loader.dataset):.4f}, "
        f"KL Divergence={kl/len(train_loader.dataset):.4f}"
    )

# -----------------------------
# FINAL SUMMARY
# -----------------------------
print("\nâœ… SUMMARY")
print("â€¢ AutoEncoder gives lowest reconstruction error but no latent regularization.")
print("â€¢ Increasing beta improves disentanglement but increases reconstruction loss.")
print("â€¢ Beta â‰ˆ 1.0 provides a balanced trade-off between reconstruction and regularization.")
